# -*- coding: utf-8 -*-
"""Anomaly_Detection_in_Network_Traffic_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19BkT2aFHV5R7wR5MQTR4qGNV6w2GX3fy
"""

from google.colab import files
uploaded = files.upload()

import zipfile
import os
zip_path = "archive (1).zip"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall("network_data")

os.listdir("network_data")

import pandas as pd

df = pd.read_csv("network_data/kddcup.data_10_percent_corrected")
df.head()

display(df.head())
display(df.info())
display(df.describe())

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Identify categorical and numerical columns
categorical_cols = df.select_dtypes(include=['object']).columns
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns

# Apply one-hot encoding to categorical columns
df_categorical_encoded = pd.get_dummies(df[categorical_cols], drop_first=True)

# Scale numerical columns
scaler = StandardScaler()
df_numerical_scaled = scaler.fit_transform(df[numerical_cols])
df_numerical_scaled = pd.DataFrame(df_numerical_scaled, columns=numerical_cols)

# Concatenate the processed features
df_processed = pd.concat([df_categorical_encoded, df_numerical_scaled], axis=1)

# Split data into training and testing sets (using 80/20 split)
# Regenerate X_test to ensure it does not have extra columns from previous runs
X_train, X_test = train_test_split(df_processed, test_size=0.2, random_state=42)

display(X_train.head())
display(X_test.head())

from sklearn.ensemble import IsolationForest

# Instantiate Isolation Forest model
iso_forest = IsolationForest(contamination='auto', random_state=42)

# Fit the model to the training data
iso_forest.fit(X_train)

# Predict anomaly scores on the test data
anomaly_scores = iso_forest.decision_function(X_test)

# Predict anomaly labels on the test data (-1 for anomalies, 1 for normal)
anomaly_labels = iso_forest.predict(X_test)

# Display the first few anomaly scores and labels
print("Anomaly Scores (first 5):")
print(anomaly_scores[:5])
print("\nAnomaly Labels (first 5):")
print(anomaly_labels[:5])

from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense
from sklearn.metrics import mean_squared_error

# Define input dimension
input_dim = X_train.shape[1]

# Build the autoencoder model
autoencoder = Sequential()
autoencoder.add(Dense(128, activation='relu', input_shape=(input_dim,)))
autoencoder.add(Dense(64, activation='relu'))
autoencoder.add(Dense(32, activation='relu'))
autoencoder.add(Dense(64, activation='relu'))
autoencoder.add(Dense(128, activation='relu'))
autoencoder.add(Dense(input_dim, activation='sigmoid')) # Output layer with sigmoid for reconstruction

# Compile the autoencoder model
autoencoder.compile(optimizer='adam', loss='mean_squared_error')

# Train the autoencoder model
history = autoencoder.fit(X_train, X_train, epochs=10, batch_size=256, shuffle=True, validation_split=0.2, verbose=0)

# Predict reconstructed data for the test set
X_test_reconstructed = autoencoder.predict(X_test)

# Calculate the mean squared error between original and reconstructed test data
reconstruction_error = mean_squared_error(X_test, X_test_reconstructed)

# Display the mean squared error
print(f"Mean Squared Error on Test Data: {reconstruction_error}")

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

reconstruction_errors = np.mean(np.power(X_test.values - X_test_reconstructed, 2), axis=1)
X_test_results = X_test.copy()
X_test_results['isolation_forest_anomaly_score'] = anomaly_scores
X_test_results['autoencoder_reconstruction_error'] = reconstruction_errors
display(X_test_results.head())

plt.figure(figsize=(10, 6))
sns.histplot(X_test_results['isolation_forest_anomaly_score'], bins=50, kde=True)
plt.title('Distribution of Isolation Forest Anomaly Scores')
plt.xlabel('Anomaly Score')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(X_test_results['autoencoder_reconstruction_error'], bins=50, kde=True)
plt.title('Distribution of Autoencoder Reconstruction Errors')
plt.xlabel('Reconstruction Error')
plt.ylabel('Frequency')
plt.show()

# Set a threshold for anomaly detection for Isolation Forest (e.g., based on visual inspection of the histogram)
# Lower scores are more anomalous for Isolation Forest
isolation_forest_threshold = X_test_results['isolation_forest_anomaly_score'].quantile(0.05) # Example: Top 5% most anomalous
X_test_results['isolation_forest_anomaly'] = X_test_results['isolation_forest_anomaly_score'] < isolation_forest_threshold

# Set a threshold for anomaly detection for Autoencoder (e.g., based on visual inspection of the histogram)
# Higher errors are more anomalous for Autoencoder
autoencoder_threshold = X_test_results['autoencoder_reconstruction_error'].quantile(0.95) # Example: Top 5% most anomalous
X_test_results['autoencoder_anomaly'] = X_test_results['autoencoder_reconstruction_error'] > autoencoder_threshold

# Display the counts of anomalies detected by each model
print("\nAnomaly detection counts:")
print("Isolation Forest Anomalies:", X_test_results['isolation_forest_anomaly'].sum())
print("Autoencoder Anomalies:", X_test_results['autoencoder_anomaly'].sum())

# Display the rows identified as anomalies by Isolation Forest
print("\nAnomalies detected by Isolation Forest (first 5):")
display(X_test_results[X_test_results['isolation_forest_anomaly']].head())

# Display the rows identified as anomalies by Autoencoder
print("\nAnomalies detected by Autoencoder (first 5):")
display(X_test_results[X_test_results['autoencoder_anomaly']].head())